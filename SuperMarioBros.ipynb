{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SuperMarioBros.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWCzM4Rln8PfeJb+5+xHfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquapathos/MLLesson/blob/master/SuperMarioBros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IluvcmqUHeFb"
      },
      "source": [
        "# SuperMarioBros openAI gym 環境などのインストール\r\n",
        "\r\n",
        "- gmy-super-mario-bros\r\n",
        "- wandb\r\n",
        "- 自作ライブラリ\r\n",
        "\r\n",
        "wandb のアカウントを持っていることが前提です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1EvzpZLG1XW"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/aquapathos/MLLesson/master/mksmb_env.py -q\r\n",
        "!pip install gym-super-mario-bros > /dev/null\r\n",
        "!pip install wandb > /dev/null\r\n",
        "!pip install stable-baselines3 > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-gi35IIHdCP"
      },
      "source": [
        "このインストール過程で、[nes-py](https://github.com/Kautenja/nes-py) というファミコンエミュレータがインストールされる模様。\r\n",
        "\r\n",
        "モデルを保存するために Google Drive をマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCyow2s0HAYq",
        "outputId": "b13b4d54-b102-46c9-89ef-bd92a6652e1d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druSn2okIMQH"
      },
      "source": [
        "# 動作確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDC0TaruH1Ou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "ba87502e-e4d3-4479-b31e-8f6b81479ded"
      },
      "source": [
        "import gym,cv2\r\n",
        "\r\n",
        "import gym_super_mario_bros\r\n",
        "from mksmb_env import Joy,make_mario_env,recordModelCallback\r\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT,RIGHT_ONLY\r\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv,SubprocVecEnv,VecFrameStack\r\n",
        "\r\n",
        "env_id = 'SuperMarioBros-v0'\r\n",
        "#env = make_mario_env(env_id, n_envs=8, seed=0,vec_env_cls=SubprocVecEnv, monitor_dir= \"test\")\r\n",
        "env = make_mario_env(Joy(), n_envs=4, seed=0,vec_env_cls=DummyVecEnv, monitor_dir= \"test\")\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "\r\n",
        "from PIL import Image \r\n",
        "Image.fromarray((env.render(mode='rgb_array')))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USEW False\n",
            "USEW False\n",
            "USEW False\n",
            "USEW False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=512x480 at 0x7F3992D52208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k4BGPEpK5vn"
      },
      "source": [
        "スーパーマリオのアクションスペースは256種ものアクションがあり、[JoypadSpace() ](https://github.com/Kautenja/nes-py/blob/master/nes_py/wrappers/joypad_space.py))というラッパーを通すと7種に減ることがわかる。[JoypadSpace() ](https://github.com/Kautenja/nes-py/blob/master/nes_py/wrappers/joypad_space.py)のソースを見ると、\r\n",
        "\r\n",
        "```\r\n",
        "    # a mapping of buttons to binary values\r\n",
        "    _button_map = {\r\n",
        "        'right':  0b10000000,\r\n",
        "        'left':   0b01000000,\r\n",
        "        'down':   0b00100000,\r\n",
        "        'up':     0b00010000,\r\n",
        "        'start':  0b00001000,\r\n",
        "        'select': 0b00000100,\r\n",
        "        'B':      0b00000010,\r\n",
        "        'A':      0b00000001,\r\n",
        "        'NOOP':   0b00000000,\r\n",
        "    }\r\n",
        "```\r\n",
        "とあり、\r\n",
        "\r\n",
        "```\r\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\r\n",
        "```\r\n",
        "\r\n",
        "は、[actions.py](https://github.com/Kautenja/gym-super-mario-bros/blob/1a3dde897700614f729fc3294c1e451528d9a112/gym_super_mario_bros/actions.py)で、\r\n",
        "\r\n",
        "```\r\n",
        "# actions for very simple movement\r\n",
        "SIMPLE_MOVEMENT = [\r\n",
        "    ['NOOP'],\r\n",
        "    ['right'],\r\n",
        "    ['right', 'A'],\r\n",
        "    ['right', 'B'],\r\n",
        "    ['right', 'A', 'B'],\r\n",
        "    ['A'],\r\n",
        "    ['left'],\r\n",
        "]\r\n",
        "```\r\n",
        "と定義されている。基本的にスーパーマリオは右スクロールのゲームなので、これでもよさげだが、左ジャンプぐらいもやはりほしい。upは NOOPと同じなので省いて、次のアクションスペースを使ってみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6lTkwft8UNo"
      },
      "source": [
        "MYACTIONSET = [\r\n",
        "    ['NOOP'],\r\n",
        "    ['right'],\r\n",
        "    ['right', 'A'],\r\n",
        "    ['right', 'B'],\r\n",
        "    ['right', 'A', 'B'],\r\n",
        "    ['A'],\r\n",
        "    ['left'],\r\n",
        "    ['left', 'A'],\r\n",
        "    ['left', 'B'],\r\n",
        "    ['left', 'A', 'B'],\r\n",
        "    ['down'],\r\n",
        "]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XBF1d299LEm"
      },
      "source": [
        "ゲーム画面のサイズは"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOlNSIXW7Qky",
        "outputId": "a11df5f4-4449-4606-c640-3c70de334f6f"
      },
      "source": [
        "gym_super_mario_bros.make('SuperMarioBros-v0').reset().shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240, 256, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2A35Fcq6cVx"
      },
      "source": [
        "# PPO学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qkHrH0k6aNu",
        "outputId": "22a2f73a-cdf9-4b19-f2eb-a5d43d2f4c3d"
      },
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv,SubprocVecEnv,VecFrameStack,VecEnv\r\n",
        "from stable_baselines3 import PPO\r\n",
        "from stable_baselines3.common.callbacks import BaseCallback\r\n",
        "import wandb,os\r\n",
        "import numpy as np\r\n",
        "import torch,gym\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(repr(device))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device(type='cuda', index=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq3uEnnaIiAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "f24b318a-f8ee-4fb5-b25a-7b1564d58639"
      },
      "source": [
        "log_dir ='/content/drive/MyDrive/M/smb'\r\n",
        "\r\n",
        "# Inside my model training code \r\n",
        "!export WANDB_NOTEBOOK_NAME=\"Mario\".ipynb\"\r\n",
        "import wandb\r\n",
        "PROJECTNAME='Mario'\r\n",
        "wandb.init(project=PROJECTNAME)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maquapathos\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">drawn-cosmos-18</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/aquapathos/Mario\" target=\"_blank\">https://wandb.ai/aquapathos/Mario</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/aquapathos/Mario/runs/1sg4ar5x\" target=\"_blank\">https://wandb.ai/aquapathos/Mario/runs/1sg4ar5x</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210111_110439-1sg4ar5x</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f39336a7630>"
            ],
            "text/html": [
              "<h1>Run(1sg4ar5x)</h1><p></p><iframe src=\"https://wandb.ai/aquapathos/Mario/runs/1sg4ar5x\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7sLq58-62b9",
        "outputId": "82aa5ae9-b65b-478f-b8d6-bf28f82119e6"
      },
      "source": [
        "monargs={'usewandb':True}\r\n",
        "#env = make_mario_env(Joy(movement=MYACTIONSET), n_envs=8, seed=0,vec_env_cls=SubprocVecEnv, monitor_dir=log_dir,monitor_kwargs=monargs)\r\n",
        "env = make_mario_env(Joy(movement=MYACTIONSET), n_envs=8, seed=0,vec_env_cls=DummyVecEnv, monitor_dir= log_dir,monitor_kwargs=monargs)\r\n",
        "env = VecFrameStack(env, n_stack=4)\r\n",
        "\r\n",
        "model = PPO('CnnPolicy', env, verbose=0,device=device,        \r\n",
        "    learning_rate = 3e-4,  # default  3e-4\r\n",
        "    n_steps = 2048, # 2048\r\n",
        "    clip_range = 0.2, # default 0.2\r\n",
        "    ent_coef = 0.01, # default 0.01\r\n",
        "    batch_size = 64, #64\r\n",
        "    n_epochs = 4 ) # 4\r\n",
        "\r\n",
        "# model.load('/content/drive/MyDrive/M/smb/best_model.zip')\r\n",
        "callback = recordModelCallback(check_freq=50, log_dir=log_dir, usewandb=True)\r\n",
        "total_timesteps = 1e8\r\n",
        "%time model.learn(total_timesteps=total_timesteps,callback=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "USEW True\n",
            "Num timesteps: 4400 : Best mean reward: 0.00 - Last mean reward/ep: 314.00\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 4800 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 5200 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 5600 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 6000 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 6400 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 6800 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 7200 : Best mean reward: 314.00 - Last mean reward/ep: 314.00\n",
            "Num timesteps: 7600 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 8000 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 8400 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 8800 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 9200 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 9600 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 10000 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 10400 : Best mean reward: 314.00 - Last mean reward/ep: 252.00\n",
            "Num timesteps: 10800 : Best mean reward: 314.00 - Last mean reward/ep: 410.67\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 11200 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 11600 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 12000 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 12400 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 12800 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 13200 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 13600 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 14000 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 14400 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 14800 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 15200 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 15600 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 16000 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 16400 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 16800 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 17200 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 17600 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 18000 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 18400 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 18800 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 19200 : Best mean reward: 410.67 - Last mean reward/ep: 410.67\n",
            "Num timesteps: 19600 : Best mean reward: 410.67 - Last mean reward/ep: 528.75\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 20000 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 20400 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 20800 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 21200 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 21600 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 22000 : Best mean reward: 528.75 - Last mean reward/ep: 528.75\n",
            "Num timesteps: 22400 : Best mean reward: 528.75 - Last mean reward/ep: 619.20\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 22800 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 23200 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 23600 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 24000 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 24400 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 24800 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 25200 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 25600 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 26000 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 26400 : Best mean reward: 619.20 - Last mean reward/ep: 619.20\n",
            "Num timesteps: 26800 : Best mean reward: 619.20 - Last mean reward/ep: 629.00\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 27200 : Best mean reward: 629.00 - Last mean reward/ep: 629.00\n",
            "Num timesteps: 27600 : Best mean reward: 629.00 - Last mean reward/ep: 717.71\n",
            "Saving new best model to /content/drive/MyDrive/M/smb/best_model.zip\n",
            "Num timesteps: 28000 : Best mean reward: 717.71 - Last mean reward/ep: 717.71\n",
            "Num timesteps: 28400 : Best mean reward: 717.71 - Last mean reward/ep: 717.71\n",
            "Num timesteps: 28800 : Best mean reward: 717.71 - Last mean reward/ep: 717.71\n",
            "Num timesteps: 29200 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 29600 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 30000 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 30400 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 30800 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 31200 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 31600 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 32000 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n",
            "Num timesteps: 32400 : Best mean reward: 717.71 - Last mean reward/ep: 700.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0UaFsOT-0Km"
      },
      "source": [
        "!rm /content/drive/MyDrive/M/smb/?.moni*"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oje1V5g0HOuu"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}