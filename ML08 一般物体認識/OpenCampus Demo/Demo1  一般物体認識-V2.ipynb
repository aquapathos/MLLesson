{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 準備\n",
    "最初データの読み込みに少し時間がかかりますので、下のブロックの中にマウスカーソルを置いて左クリックし、 **Shift キーを押しながら Enter**を押しておいてください。Shift+Enter でそのブロックのプログラムが実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysrc.placenetVgg2 as pp                  #   枠内でShift を押しながらEnterしてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一般物体認識\n",
    "(このデモプログラムはChromeブラウザでご覧ください）\n",
    "\n",
    "写真や絵の中に写っているものが何かをこたえることは皆さんには特に難しいことではありませんね。\n",
    "\n",
    "わたしたちはパッと一目見ただけで見たものが何であるかを認識できます。例えば下の写真がどういうシーンを写したものか、誰でもすぐにわかります。\n",
    "\n",
    "![](https://goo.gl/yC2Enf)\n",
    "\n",
    "人がホームを歩いていることや、電車が停まっていることも、小さくて見えないけども時計があることもわかりますし、一番手前の人がおそらく女性であり、帽子を被っていること、傘を持っていること、だからこの日は雨だったのかな、というところまでわかります。\n",
    "\n",
    "このようなモノの認識は**一般物体認識**とよばれます。工場で決まったものをを工作するロボットや、ロボットサッカーのロボットは、それはそれでとても難しい技術から成り立っていますが、彼らは特定のものだけを見て認識したらいいだけなので、「認識」処理自体は一般物体認識と比べればとても簡単です。（それでも十分難しい。）　\n",
    "\n",
    "最近よく話題になる自動運転でさえ、一般物体認識と比べればずいぶん簡単です。道路上にあるものだけを対象とすればいいですし、障害物かそうでないか、それがどう障害になるかだけわかればいいですから。\n",
    "\n",
    "これまでコンピュータは**パターン認識**が苦手だとされてきました。とりわけ一般物体認識は苦手でした。\n",
    "\n",
    "## ILSVRC \n",
    "ILSVRCは一般物体認識の精度を競う競技会で、2010年から始まりました。当初は従来の画像認識技術を踏まえたさまざまなアプローチで競われていましたが、2012年にトロント大学のチームがディープニューラルネットを用いたプログラムでぶっちぎり優勝して以来、最近はほとんどの参加者がディープラーニングという人工知能技術を利用しています。\n",
    "\n",
    "この大会の成果の多くは公開されて再利用可能となっています。\n",
    "\n",
    "以下でお見せするデモプログラムは、そのうちのひとつである、オックスフォード大学の研究チームが公開している学習済みモデル（2014準優勝）を利用して作りました。\n",
    "\n",
    "\n",
    " ||||\n",
    " |:--:|:--:|:--:|:--:|\n",
    " |　　|<img src=\"https://goo.gl/fCecNj\" width=\"450\">|<img src=\"https://goo.gl/t4JUfK\" width=\"300\">|　　|\n",
    "\n",
    "- [ILSVRC カテゴリ](http://localhost:8888/edit/ML08%20一般物体認識/OpenCampus%20Demo/pysrc/modeldata/jcategories.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: red;\">  ☆☆スタート☆☆</span>\n",
    "# 実験1　画像内容識別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive\n",
    "interact(pp.predict, url=\"https://goo.gl/ZiirT2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    " url のボックスの中に認識させたい画像の URL をはりつけてEnterすれば認識結果が表示されます。\n",
    " \n",
    " <hr>\n",
    "\n",
    "### テスト画像\n",
    "\n",
    "||||||||\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|![](https://goo.gl/qhnMce)|<img src=\"https://goo.gl/j6zq9g\" width=\"600\">|![](https://goo.gl/rKzvJw)|![](https://goo.gl/c4YGXB)|![](https://goo.gl/SGneEZ)|![](https://goo.gl/cz7pgE)|<img src=\"https://goo.gl/853iBW\" width=\"512\">|<img src=\"https://goo.gl/dJTemk\" width=\"256\">|![](https://goo.gl/eaVAeu)|![](https://goo.gl/7qLQXW)|\n",
    "↑ 右クリックして「画像アドレスをコピー」してください。\n",
    "\n",
    "### google 画像検索\n",
    "\n",
    "||||||\n",
    "|--:|:--:|:--:|:--:|:--:|\n",
    "|右クリックし、新しいウィンドウで開いて下さい→|<a href=\"https://goo.gl/zeb5pM\"> ![動物](https://user-images.githubusercontent.com/5820803/30189909-c54acd10-9471-11e7-809e-5731dc3d239b.PNG)</a>|<a href=\"https://goo.gl/dZ7Vp2\">![植物](https://user-images.githubusercontent.com/5820803/30190043-93863f20-9472-11e7-8dda-bd50c807f20a.PNG)</a>|<a href=\"https://goo.gl/FGnvS1\">![爬虫類](https://user-images.githubusercontent.com/5820803/30189856-8806e40c-9471-11e7-9a4a-adfd718307b0.PNG)</a>|<a href=\"https://goo.gl/yuDj9L\">![建造物](https://user-images.githubusercontent.com/5820803/30190014-658082d4-9472-11e7-96d8-0cbc43b105de.PNG)</a>|\n",
    "||||||\n",
    "\n",
    "\n",
    "対象としたい画像を選んでクリック、画像右横に出る　** 「画像を表示」 ** のボタンを押してその画像のみを表示し、右クリックメニーの「画像アドレスをコピー」するか、ブラウザ上部に表示されている URL をコピーしてください。\n",
    "\n",
    "### <a href=\"http://image-net.org/explore\" target=\"_blank\">ImageNet</a> \n",
    "ジャンルを検索窓に入力するか（ただし英語）、左の分類ツリーをクリックしていくと画像が表示されますので、画像が一つだけになるまでクリックを繰り返し、右クリックメニーの「画像アドレスをコピー」するか、ブラウザ上部に表示されている URL をコピーしてください。\n",
    "\n",
    " <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像のURLについての注意\n",
    "URL の末尾が jpg や png などの画像の拡張子であることを確認してから下の url の箱にペーストしてください。\n",
    " サイトによっては画像へのアクセスが拒否される場合もあります。エラーが出た場合は別の URL  でチャレンジしてみてください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験準備１　グレイ化\n",
    "カラー画像をモノクロ画像に変換する処理はもっとも簡単な画像処理のひとつです．\n",
    "\n",
    "$$ Gray = ( 0.298912 R + 0.586611  G + 0.114478 B ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy\n",
    "from PIL import Image\n",
    "gray  = cv2.cvtColor(numpy.asarray(pp.pubimg), cv2.COLOR_BGR2GRAY)\n",
    "Image.fromarray(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**画像が表示されたら** 右クリックでデスクトップに保存してください．名前はなんでもかまいません．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験準備２　線画化\n",
    "エッジ検出という画像処理を施せば線画化できます．エッジ検出はパターン認識の手掛かりをえるためのもっとも基本的な処理です．\n",
    "\n",
    "エッジ検出の基本は高校の数IIで習う**微分**です。明るさをｘとｙの関数ととらえ、ｘ、ｙで微分するとそれぞれの方向に沿った明るさの傾きが得られます。エッジの位置では明るさが大きく変化するので微分値は大きく、明るさの変化の少ない場所では微分値は０に近くなります。ものの輪郭は明るさの変化が大きいので微分値は大きくなります。微分値の大きな点を黒で描いていけば線画っぽいものができあがります。\n",
    "\n",
    "<img src=\"https://goo.gl/6TtxwQ\" width=20%>\n",
    "\n",
    "ただ、一般のシーンでは明るさの変化が複雑すぎるので、そのまま微分すると上の図のようにエッジが出すぎて線画として使うのに向きません。そこで、次のプログラムではまず画像をわざとぼかしてからエッジ検出を施しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.GaussianBlur(gray, (5,5), 0)  # 　ガウスぼかし\n",
    "pic_edges = cv2.bitwise_not(cv2.Canny(gray, threshold1=20, threshold2=60))  # エッジ検出して白黒反転\n",
    "Image.fromarray(cv2.cvtColor(pic_edges, cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**画像が表示されたら**  右クリックでデスクトップに保存してください．名前はなんでもかまいませんが、先に保存したグレイ画像とは別の名前にしてください．.\n",
    "**ノイズが多いと感じる場合**はShift+Enterを何度か繰り返してみみててください．実行するたびに線が減っていきます。\n",
    "減らしすぎてしまった場合はグレイ化のブロックからやり直してください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験２　自動着色"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href=\"http://hi.cs.waseda.ac.jp:8082\" target=\"_brank\"> 白黒写真の自動色付け </a>\n",
    "- <a href=\"https://paintschainer.preferred.tech/index_ja.html\" target=\"_blank\"> Paint Chainer 線画の色付け</a>\n",
    "\n",
    "この二つは外部サイトです．これらの処理には，いずれもディープニューラルネットワークという技術が使われています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソースコード\n",
    "ソースプログラムを下に示しておきます。学習済みモデルを用いているので認識自体は\n",
    "\n",
    "> ```y, = vgg(inputs={'data': x}, outputs=['fc8a'])```\n",
    "\n",
    "の1行です。ｘが画像で、ｙに1365あるカテゴリすべての可能性を表す数値のリストが返ってきます。\n",
    "\n",
    "この行以外の部分はネット上の画像を読み込んで形式を合わせたり、上位5位までを取り出して単語として表示したりするためのプログラムコードになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃　ソースコード   placenetVgg2.py\n",
    "```\n",
    "# import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import Variable\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import io\n",
    "import urllib.request\n",
    "\n",
    "from chainer.links.model.vision.vgg import prepare as VGGprepare\n",
    "import pickle\n",
    "\n",
    "vgg = pickle.load(open('pysrc/modeldata/vgg16_hybrid1365.pkl', 'rb'))\n",
    "\n",
    "mean = np.array([103.939, 116.779, 123.68])   # BGR\n",
    "# blob データを PIL 画像に変\n",
    "def blob2img(blob, mean=mean):\n",
    "    blob = (np.dstack(blob)+ mean)[:,:,::-1]   # BGR 2 RGB\n",
    "    return PIL.Image.fromarray(np.uint8(blob))\n",
    "\n",
    "# 確率リストとしての出力からトップ５を出力するメソッド2\n",
    "# 日本語化済みのカテゴリリストを用いる\n",
    "f = open(\"pysrc/modeldata/jcategories.txt\",'r',encoding=\"utf-8\")\n",
    "jcategories={}\n",
    "for n in range(1365):\n",
    "    jcategories[n]=f.readline()[:-1]\n",
    "f.close()\n",
    "\n",
    "def showtop2(prob, ranklimit=5): # prob は最終層から出力される確率リスト（Variable構造体)\n",
    "    top5args = np.argsort(prob.data)[:-ranklimit-1:-1] # 上位５つの番号\n",
    "    top5probs = prob.data[top5args] # 上位５つの確率\n",
    "    for rank,(n, p) in enumerate(zip(top5args,top5probs)):\n",
    "        print(\"{} {} ({:7.5f})\".format(rank+1,jcategories[n], top5probs[rank]))\n",
    "\n",
    "def url2img(url):\n",
    "    # print(url)\n",
    "    if url[:16] == \"http://localhost\":\n",
    "        pic = url.rsplit('/',1)[1]\n",
    "        f = open(\"pics/\"+pic,'rb')\n",
    "    elif url[:4] != \"http\":\n",
    "        f = open(url,'rb')\n",
    "    else:\n",
    "        f = io.BytesIO(urllib.request.urlopen(url).read())\n",
    "    img = PIL.Image.open(f)\n",
    "    w,h = img.width, img.height\n",
    "    if w > h:\n",
    "        w1, h1 = int(448/h * w), 448\n",
    "    else :\n",
    "        w1,h1 = 448, int(448/w * h)\n",
    "    return img.resize((w1,h1))\n",
    "\n",
    "def predict(url=\"\"):\n",
    "    global pubimg\n",
    "    if len(url) < 10 :  # おそらく操作ミスの場合\n",
    "        return np.zeros((3,244,244))\n",
    "    pubimg = url2img(url)\n",
    "    x = Variable( VGGprepare(pubimg)[np.newaxis,])\n",
    "    y, = vgg(inputs={'data': x}, outputs=['fc8a'])\n",
    "    predict = F.softmax(y)\n",
    "    showtop2(predict[0])\n",
    "    return pubimg\n",
    "```\n",
    "\n",
    "\n",
    "![](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
    "![vgg16](https://user-images.githubusercontent.com/5820803/30191230-6843cc62-947a-11e7-8313-8dd787357f97.png)\n",
    "\n",
    "\n",
    "### 学習済みモデル\n",
    "このプログラムでは、MITが後悔している　　place365hybrid という、一般の物体とシーンを対象とした学習済み CNN(畳み込みネットワーク）を使っています。　https://github.com/CSAILVision/places365\n",
    "\n",
    "このネットワークは、、一般物体認識は ILSCVR とよばれるコンテストで用いられている 1000カテゴリと Place Net の 365のカテゴリを合わせた 1365 のカテゴリについて、与えられた画像の被写体である確率を出力します。このプログラムでは確率の上位5つを表示しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/5820803/29495822-f3616386-8601-11e7-9844-2fcd1a249468.gif)\n",
    "\n",
    "# 関連するトピックス\n",
    "\n",
    "- [派生プログラム（淡）](https://gist.github.com/aquapathos/e7bb81a3bdd1a97f9337df4bc493a15a) 上のヒマワリの絵を作ったプログラム\n",
    "- [写真に絵画風に加工するプログラム（淡）](https://research.preferred.jp/2015/09/chainer-gogh/)  \n",
    "- [画風を変換するアルゴリズム](https://research.preferred.jp/2015/09/chainer-gogh/)\n",
    "- [Google Deep Dream](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
